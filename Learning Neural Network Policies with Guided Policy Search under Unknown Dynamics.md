## Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics
##### Sergey Levine and Pieter Abbeel

This paper talks about optimizing linear gaussian time varying policies to learn a global parameterized policy such as a neural network. Also, if the covariance of the gaussian policy is narrow, then the policy induces approximately gaussian distribution over trajectories.

The gaussian controllers can be written as a gaussian with mean as the deterministic solution to the iLQR problem and covariance as the inverse of the Q function in the linear dynamics, quadratic cost formulation. The linear gaussian dynamics can be estimated using the samples generated by the controller. One of the issues here is that the iLQR solutions are only accurate as long as the model dynamics are accurate. Therefore, the objective function needs to be modified so as to incorporate a constraint on the the change in trajectory distribution once a control signal is applied. This is done by adding a KL divergence term between the old and new trajectory distributions. Such a constraint ensures that the agent does not land up in states which are too different from what it is used to (for which the dynamics are accurate enough). Also, to even reduce the number of samples for training, a GMM prior can be introduced on the gaussian dynamics. 

A parametrized policy, such as a neural network policy can then be learned to match the samples from the linear controllers. The linear controllers are in turn optimized to minimize the expected cost and the difference from the curent policy.